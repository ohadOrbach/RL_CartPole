# -*- coding: utf-8 -*-
"""Copy of CartPole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y8TTITB_7Xs9um4NljKscFI9joLSwSmW

# Deep Q-Network for Cart-Pole.

In this assignment you are required to implement a Deep Q-Network agent, for the Cart-Pole problem.

You need to submit your code, along with a plot of the rewards during training, and a video of an episode with the trained agent.

__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you'll find it easy to adapt it to almost any Python-based deep learning framework.
"""

!pip install stable-baselines3 gym==0.22.0
!pip install pyglet==1.5.27
!pip install pyvirtualdisplay

import os
import random
import numpy as np
import torch
import gym
import numpy as np
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay
from pyvirtualdisplay import Display

if type(os.environ.get("DISPLAY")) is not str or len(os.environ.get("DISPLAY")) == 0:
    !apt-get -qq install -y xvfb
    os.environ['DISPLAY'] = ':1'

display = Display(visible=0, size=(400, 300))
display.start()

"""## Cart Pole

In this environment a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.

The observation consists of the cart position, cart velocity, pole angle and pole angular velocity.

There are 2 actions corresponding to moving the cart to the left and to the right.

See full description [here](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).


"""

ENV_NAME = "CartPole-v0"

def make_env(seed=None):
    # some envs are wrapped with a time limit wrapper by default
    env = gym.make(ENV_NAME).unwrapped
    if seed is not None:
        env.seed(seed)
    return env

env = gym.make(ENV_NAME)
obs = env.reset()


n_actions = env.action_space.n
obs_dim = obs.shape
print("observation dim", obs_dim, "number of actions", n_actions)

n_cols = 5
n_rows = 2
fig = plt.figure(figsize=(16, 9))

for row in range(n_rows):
    for col in range(n_cols):
        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)
        ax.imshow(env.render('rgb_array'))
        env.step(env.action_space.sample())
plt.show()

"""## Random Agent

Here we implement a random agent to use in the 'evaluate' function, that runs the agent through multiple epsiodes.
"""

class RandomAgent():
    def __init__(self, n_actions):
        self.n_actions = n_actions


    def get_qvalues(self, states):
        return np.random.normal(size=(1,self.n_actions,))

    def sample_actions(self, qvalues):
        return np.random.randint(self.n_actions, size=(1,))

def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):
    """ Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. """
    rewards = []
    for _ in range(n_games):
        s = env.reset()
        reward = 0
        for _ in range(t_max):
            qvalues = agent.get_qvalues([s])
            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]
            s, r, done, _ = env.step(action)
            reward += r
            if done:
              break

        rewards.append(reward)
    return np.mean(rewards)

agent = RandomAgent(n_actions)
mean_reward = evaluate(env, agent, greedy=True, n_games=5)

"""## Video generation"""

# run sessions

import gym.wrappers
import sys
from pathlib import Path
from base64 import b64encode
from IPython.display import HTML

n_games = 1 # how many games to record

with gym.wrappers.Monitor(make_env(), directory="videos", force=True) as env_monitor:
    env_monitor.reset()
    sessions = [evaluate(env_monitor, agent, n_games=3, greedy=False) for _ in range(10)]

# show video

video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])
video_path = video_paths[1]  # You can also try other indices (if n_games>1)

if 'google.colab' in sys.modules:
    # https://stackoverflow.com/a/57378660/1214547
    with video_path.open('rb') as fp:
        mp4 = fp.read()
    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()
else:
    data_url = str(video_path)

HTML("""
<video width="640" height="480" controls>
  <source src="{}" type="video/mp4">
</video>
""".format(data_url))

"""## Assignment

Implement a DQN agent that contains:
1. A neural network that predicts the q-values for a given observation.
2. An experience replay buffer where all transitions are added.
3. A target Q-network that is formed by periodically copying the weights of the main Q-nework.

You will also need to implement the training loop, and submit a figure showing the reward as a function of the training step, and a video showing an episode with the trained agent.
"""

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
from torch import optim
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""# **Network: DQN**"""

class DQN(nn.Module):
    def __init__(self, n_observations=4, hidden_size=128, n_actions=2):
        super(DQN, self).__init__()
        self.l1 = nn.Linear(n_observations, hidden_size)
        self.relu = nn.ReLU()
        self.l2 = nn.Linear(hidden_size, n_actions)

    def forward(self, x):
        out = self.l1(x)
        out = self.relu(out)
        out = self.l2(out)
        return out

"""# DQNAgent - Policy: $\epsilon$-Greedy"""

class DQNAgent():
  def __init__(self, initial_state):
    self.n_actions = 2
    self._epsilon = 0.05
    self._action = 0
    self._replay_buffer = []
    self._initial_state = initial_state
    self.step_counter = 0
    self.step_size = 0.1
    self.batch_size = 128

    self._epsilon_start = 1.0
    self._epsilon_end = 0.05
    self._epsilon_decay = 0.995
    self._epsilon = epsilon_start  # Initialize epsilon to start value

    #  A neural network that predicts the q-values for a given observation.
    self.main_q_network = DQN()
    self.optimizer = torch.optim.AdamW(self.main_q_network.parameters(), lr=0.0005)

    # A target Q-network
    self.target_q_network = DQN()

    # loss function MSE
    self.criterion = nn.MSELoss()

  def update_epsilon(self, episode):
        self._epsilon = max(self._epsilon_end, self._epsilon_start * (self._epsilon_decay ** episode))

  def reset_epsilon(self):
        self._epsilon = self._epsilon_start

  def behaviour_policy(self, previous_action):     # Epsilon Greedy
    success = bool(np.random.uniform(0,1) > self._epsilon)
    if success:
      action = torch.argmax(previous_action)
    else:
        action = np.random.randint(self.n_actions)
    return action




  def sample_actions(self, qvalues):
      return self.behaviour_policy(self.main_q_network(qvalues))


  def replay(self, mini_batch, Q, discount,):
    if len(self._replay_buffer) < self.batch_size:
            return
    if len(self._replay_buffer) < mini_batch:
      n = len(self._replay_buffer)
    else:
      n = mini_batch

    for i in range(Q):      # prefomre Q learning

      self.optimizer.zero_grad()

      # Draw mini batch from relay buffer
      samples = random.sample(self._replay_buffer, self.batch_size)
      states, actions, rewards, next_states = zip(*samples)
      states = torch.tensor(states)
      actions = torch.tensor(actions)
      rewards = torch.tensor(rewards)
      next_states = torch.tensor(np.array(next_states))


      # Get max action from main_q_network
      next_q_values = self.target_q_network(torch.as_tensor(next_states)).max(1)[0].detach()
      target_q_values = rewards + discount * next_q_values

      # Get action from main_q_network
      q_values = self.main_q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

      # Calculate loss
      loss = self.criterion(q_values, target_q_values)


      # Updating parameters
      loss.backward()
      self.optimizer.step()



  def train(self, env, n_games = 10000, n_experience = 1, t_max = 10000, mini_batch=100, Q=1):
    rewards = []
    all_rewards = []

    for i in range(n_games):
      self._initial_state = env.reset()       # Start a game
      total_rewards = 0
      reward = 1
      done = 0
      action = self.behaviour_policy(self.main_q_network(torch.as_tensor(self._initial_state)))
      self.replay()

      for _ in range(t_max):
        self.step_counter += 1
        next_state, reward, done, _ = env.step(int(action))
        self._replay_buffer.append((self._initial_state, action, reward, next_state))
        self._initial_state = next_state
        action = self.behaviour_policy(self.main_q_network(torch.as_tensor(next_state)))

        if self.step_counter  % 1000 == 0:      # periodically copying the weights of the main Q-nework to target
          self.target_q_network.load_state_dict(self.main_q_network.state_dict())

        total_rewards += reward
        if done:
          break

        self.replay(mini_batch, Q, discount = 0.99)

      if len(rewards) >= 150 and np.mean(rewards) >= 400:
        break

      rewards.append(total_rewards)
      all_rewards.append(total_rewards)

      if i % 200 == 0:
        print("Epoch: {}, mean rewards: {}, mean all rewards: {}".format(i, np.mean(rewards), np.mean(all_rewards)))
        rewards = []
    self.update_epsilon(i)
    #Plot results of rewards
    plt.figure()
    plt.title('Final Rewards')
    plt.plot(all_rewards, label = 'Rewards')
    plt.xlabel('episodes')
    plt.ylabel('rewards')
    plt.show()
    return agent

env = gym.make(ENV_NAME)
agent = DQNAgent(env.reset())
agent = agent.train(env) #train agent